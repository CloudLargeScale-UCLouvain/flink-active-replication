{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install minio\n",
    "!pip install pandarallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import BytesIO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from minio import Minio\n",
    "from minio.error import NoSuchKey\n",
    "from matplotlib.cbook import boxplot_stats\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm as tqdm\n",
    "from collections import OrderedDict\n",
    "#from pandarallel import pandarallel\n",
    "#pandarallel.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "minio = Minio('flink-minio.default:9000',\n",
    "              access_key='AKIAIOSFODNN7EXAMPLE',\n",
    "              secret_key='wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY',\n",
    "              secure=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bettercloud',\n",
       " 'bettercloud-finold',\n",
       " 'bettercloud-finold2',\n",
       " 'bettercloud-finold3',\n",
       " 'bettercloud-interval',\n",
       " 'bettercloud-pre',\n",
       " 'bettercloud-pre2',\n",
       " 'bettercloud-pre3',\n",
       " 'bettercloud-pre4',\n",
       " 'bettercloud-test',\n",
       " 'bettercloud-test1',\n",
       " 'bettercloud-test2',\n",
       " 'bettercloud-test3',\n",
       " 'bettercloud-v2-2',\n",
       " 'bettercloud-v2-3',\n",
       " 'bettercloud-v2-final',\n",
       " 'bettercloud-weird',\n",
       " 'bettercloud2',\n",
       " 'fail1',\n",
       " 'fail1-2',\n",
       " 'fail1-3',\n",
       " 'final',\n",
       " 'liverobin-mb1',\n",
       " 'liverobin-mb1-test',\n",
       " 'liverobin-mb1-test2',\n",
       " 'liverobin-xp',\n",
       " 'liverobintest',\n",
       " 'm4b-old',\n",
       " 'mb1-finold',\n",
       " 'mb1-kafka-1',\n",
       " 'mb1-v2-debug',\n",
       " 'mb1-v2-final',\n",
       " 'mb1-v2-noprom',\n",
       " 'mb2-finold',\n",
       " 'mb2-pre',\n",
       " 'mb2-v2-final',\n",
       " 'mb3-finold',\n",
       " 'mb3-test',\n",
       " 'mb3-test2',\n",
       " 'mb3-test3',\n",
       " 'mb3-v2-debug',\n",
       " 'mb3-v2-final',\n",
       " 'mb3-v2-finold',\n",
       " 'mb3-v2-new',\n",
       " 'mb4',\n",
       " 'mb4-finold',\n",
       " 'mb4-old-test',\n",
       " 'mb4-other',\n",
       " 'mb4-other2',\n",
       " 'mb4-pre',\n",
       " 'mb4-test',\n",
       " 'mb4-v2-final',\n",
       " 'mb4b-simple1',\n",
       " 'mb4b-simple2',\n",
       " 'mb4b-test',\n",
       " 'mb4b-test2',\n",
       " 'mb4b-test3',\n",
       " 'mb4b-v2-final',\n",
       " 'mb4b2',\n",
       " 'mb4b2b',\n",
       " 'mb4b2c',\n",
       " 'mb4b2d',\n",
       " 'mb4c-finold',\n",
       " 'mb4c-test',\n",
       " 'mb4c-v2-final',\n",
       " 'mb4c-v2-test',\n",
       " 'mb4c-v2-test2',\n",
       " 'pre',\n",
       " 'test-bettercloud',\n",
       " 'test99',\n",
       " 'test99-2',\n",
       " 'test99-3',\n",
       " 'test99-4',\n",
       " 'test99-5',\n",
       " 'test99-6',\n",
       " 'test99-check',\n",
       " 'test99-prom',\n",
       " 'test99-prom2',\n",
       " 'test99-prom3',\n",
       " 'test99-prom4',\n",
       " 'test99-prom5']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[b.name for b in minio.list_buckets()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# generic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def load_metadata(bucket, path, mode=None):\n",
    "    infos = {}\n",
    "    with minio.get_object(bucket, path+ \"info.txt\") as f:\n",
    "        for bLine in f.readlines():\n",
    "            line = bLine.decode(\"utf-8\")        \n",
    "            key = line.split(\"=\")[0].strip().replace(\"--\",\"\")\n",
    "            value = line.split(\"=\")[1].strip()\n",
    "            if mode is not None:\n",
    "                if key in mode:\n",
    "                    infos[key] = value\n",
    "            else:\n",
    "                infos[key] = value\n",
    "    return infos\n",
    "def rename_algorithm(algorithm):\n",
    "\n",
    "    if \"LEADER\" in algorithm:\n",
    "        algorithm = \"Kazoo\"\n",
    "    elif \"BIAS\" in algorithm:\n",
    "        algorithm = \"TimeMerge\"\n",
    "    elif \"VANILLA_NOCP\" in algorithm:\n",
    "        algorithm = \"No Replication\"        \n",
    "    elif \"VANILLA\" in algorithm:\n",
    "        algorithm = \"Passive Replication\"\n",
    "    elif \"NO_ORDERING\" in algorithm:\n",
    "        algorithm = \"No ordering\"\n",
    "    elif \"LIVE_ROBIN\" in algorithm:\n",
    "        algorithm = \"Live robin\"\n",
    "    else:\n",
    "        raise Exception(\"Invalid algorithm\")\n",
    "\n",
    "    return algorithm\n",
    "def load_measurement(bucket, path, cols=[\"latency\", \"algorithm\", \"length\"]):\n",
    "    names = [\"source\", \"createdMs\", \"createdNs\", \"receivedMs\", \"receivedNs\", \"latency\"]\n",
    "    file = path + \"sink.csv\"\n",
    "\n",
    "    # Read Info.txt containing metadata\n",
    "    infos = {}\n",
    "    with minio.get_object(bucket, path+ \"info.txt\") as f:\n",
    "        for bLine in f.readlines():\n",
    "            line = bLine.decode(\"utf-8\")        \n",
    "            key = line.split(\"=\")[0].strip().replace(\"--\",\"\")\n",
    "            value = line.split(\"=\")[1].strip()\n",
    "            infos[key] = value\n",
    "\n",
    "    # Read (if exists) kill-result.csv and add values\n",
    "    kill = {}\n",
    "    try:\n",
    "        f = minio.get_object(bucket, path+ \"kill-result.csv\") \n",
    "        for bLine in f.readlines():\n",
    "            line = bLine.decode(\"utf-8\") \n",
    "            kill[line.split(\",\")[0].strip()] = line.split(\",\")[1].strip()\n",
    "            infos[line.split(\",\")[0].strip()] = line.split(\",\")[1].strip()\n",
    "    except NoSuchKey:\n",
    "       pass         \n",
    "    ar_sink = []\n",
    "    for sink in minio.list_objects_v2(bucket, path + \"sink\"):\n",
    "        file = sink.object_name\n",
    "        \n",
    "        try:\n",
    "            df = pd.read_csv(minio.get_object(bucket, file), names=names, usecols=[\"createdMs\",\"latency\",\"receivedMs\"])\n",
    "        except:\n",
    "            print(\"Unexpected error:\", sys.exc_info()[0])\n",
    "            return None\n",
    "        ar_sink.append(df)\n",
    "    if len(ar_sink) == 0:\n",
    "        print(\"Empty dataframe for path\", path)\n",
    "\n",
    "    df = pd.concat(ar_sink, ignore_index=True)\n",
    "\n",
    "    if \"start_time_initial\" in infos:\n",
    "        st = infos[\"start_time_initial\"]\n",
    "        infos[\"killS_other\"] = float(infos[\"kill_date\"]) - float(st)\n",
    "        infos[\"killS_fail\"] = float(infos[\"failing_time_initial\"]) - float(st)\n",
    "        infos[\"killS\"] = float(infos[\"end_time_initial\"]) - float(st)\n",
    "    else:\n",
    "        st = infos[\"start_time\"]\n",
    "    df[\"latencyMs\"] = (df.latency / 1000000).astype(\"float32\")\n",
    "    df[\"createdS\"] = df[\"createdMs\"] / 1000 - float(st)\n",
    "    df[\"receivedS\"] = df[\"receivedMs\"] / 1000 - float(st)\n",
    "    #df[\"createdS\"] = df[\"createdMs\"] / 1000 - float(infos[\"start_time_initial\"])\n",
    "    #infos[\"killS\"] = float(infos[\"end_time\"]) - float(infos[\"start_time_initial\"])\n",
    "    \n",
    "    infos[\"path\"] = path\n",
    "    df[\"algorithm\"] = [infos[\"algorithm\"] for i in range(0, len(df))]\n",
    "    df[\"algorithm\"] = df[\"algorithm\"].astype('category')\n",
    "    df[\"createdS_int\"] = df[\"createdS\"].apply(np.ceil).astype('int16')\n",
    "    df[\"receivedS_int\"] = df[\"receivedS\"].apply(np.ceil).astype('int16')\n",
    "    \n",
    "\n",
    "    return df[cols], infos\n",
    "\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_all(bucket, modes, warmup):\n",
    "    measurements = [o.object_name for o in minio.list_objects_v2(bucket)]\n",
    "    measurement_by_mode = {}\n",
    "    for measurement in tqdm(measurements, desc=\"Metadata\"):\n",
    "        infos = load_metadata(bucket, measurement, modes)\n",
    "        if json.dumps(infos) not in measurement_by_mode:\n",
    "            measurement_by_mode[json.dumps(infos)] = []\n",
    "        measurement_by_mode[json.dumps(infos)].append(measurement)\n",
    "    df_stats_run = []\n",
    "    df_stats = []\n",
    "    group=None\n",
    "    for group in tqdm(measurement_by_mode.keys(), desc=\"Data\"):\n",
    "        map_group = json.loads(group)\n",
    "        df_array = []\n",
    "        for measurement in measurement_by_mode[group]:\n",
    "\n",
    "            df, infos = load_measurement(bucket, measurement, [\"createdS_int\",\"latencyMs\"])\n",
    "            df_array.append(df)\n",
    "            stats = boxplot_stats(df[df[\"createdS_int\"] > warmup].latencyMs, whis=[1,99])[0]\n",
    "            stats[\"measurement\"] = measurement\n",
    "            for key in map_group:\n",
    "                stats[key] = map_group[key]\n",
    "            if \"algorithm\" in stats:\n",
    "                stats[key] = rename_algorithm(stats[key])\n",
    "            df_stats_run.append(stats)\n",
    "\n",
    "        df = pd.concat(df_array)\n",
    "        stats = boxplot_stats(df[df[\"createdS_int\"] > warmup].latencyMs, whis=[1,99])[0]\n",
    "        for key in map_group:\n",
    "            stats[key] = map_group[key]\n",
    "        if \"algorithm\" in stats:\n",
    "            stats[key] = rename_algorithm(stats[key])\n",
    "        df_stats.append(stats)\n",
    "        \n",
    "    df_stats=pd.DataFrame(df_stats)    \n",
    "    df_stats_run=pd.DataFrame(df_stats_run)  \n",
    "    return df_stats, df_stats_run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# mb1 generic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "#df, infos = load_measurement(\"test99-6\", \"2021-07-06-18-29-29-test-test99-no-1-0-0-flink-job/\", [\"createdS_int\",\"latencyMs\"])\n",
    "df, infos = load_measurement(\"test99-prom4\", \"2021-07-07-09-25-33-test-test99-no-1-0-0-flink-job/\", [\"createdS_int\",\"latencyMs\"])\n",
    "df[df.createdS_int >= 30].plot(kind=\"scatter\", x=\"createdS_int\",y=\"latencyMs\",s=1, ax=ax)\n",
    "ax.set_ylim([0,200])\n",
    "df[df.createdS_int >= 30][\"latencyMs\"].describe(percentiles=[0.001,0.01,0.1,0.5,0.9,0.99,0.999])\n",
    "plt.figure()\n",
    "df[df.createdS_int >= 30].groupby(\"createdS_int\")[\"latencyMs\"].count().plot()\n",
    "df_no = df[df.createdS_int >= 30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "#df, infos = load_measurement(\"test99-6\", \"2021-07-06-18-14-52-test-test99-nc-1-0-0-flink-job/\", [\"createdS_int\",\"latencyMs\"])\n",
    "df, infos = load_measurement(\"test99-prom4\", \"2021-07-07-09-34-46-test-test99-nc-1-0-0-flink-job/\", [\"createdS_int\",\"latencyMs\"])\n",
    "df[df.createdS_int >= 30].plot(kind=\"scatter\", x=\"createdS_int\",y=\"latencyMs\",s=1,ax=ax)\n",
    "ax.set_ylim([0,200])\n",
    "df[df.createdS_int >= 30][\"latencyMs\"].describe(percentiles=[0.001,0.01,0.1,0.5,0.9,0.99,0.999])\n",
    "plt.figure()\n",
    "df[df.createdS_int >= 30].groupby(\"createdS_int\")[\"latencyMs\"].count().plot()\n",
    "df_nc = df[df.createdS_int >= 30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_no[\"latencyMs\"].describe(percentiles=[0.001,0.01,0.1,0.5,0.9,0.99,0.999,0.9999])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nc[\"latencyMs\"].describe(percentiles=[0.001,0.01,0.1,0.5,0.9,0.99,0.999,0.9999])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "fig = df_no[df_no[\"latencyMs\"] >= df_no[\"latencyMs\"].quantile(0.999)].plot(kind=\"scatter\", x=\"createdS_int\",y=\"latencyMs\",s=1 ,figsize=(16,4),ax=ax)\n",
    "ax.set_xticks(range(30,120,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nc[\"latencyMs\"].describe(percentiles=[0.001,0.01,0.1,0.5,0.9,0.99,0.999])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e3c465ba3f442dfb2261e221723a220",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Metadata', max=2.0, style=ProgressStyle(description_width…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cca239c9dc44284adafc101eaf9fd53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Data', max=2.0, style=ProgressStyle(description_width='in…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>iqr</th>\n",
       "      <th>cilo</th>\n",
       "      <th>cihi</th>\n",
       "      <th>whishi</th>\n",
       "      <th>whislo</th>\n",
       "      <th>fliers</th>\n",
       "      <th>q1</th>\n",
       "      <th>med</th>\n",
       "      <th>q3</th>\n",
       "      <th>rate</th>\n",
       "      <th>algorithm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.749423</td>\n",
       "      <td>2.992702</td>\n",
       "      <td>5.626825</td>\n",
       "      <td>5.64040</td>\n",
       "      <td>10.372989</td>\n",
       "      <td>1.743512</td>\n",
       "      <td>[1.697545, 1.672757, 1.47343, 1.658997, 1.6082...</td>\n",
       "      <td>4.156760</td>\n",
       "      <td>5.633612</td>\n",
       "      <td>7.149461</td>\n",
       "      <td>5000</td>\n",
       "      <td>No Replication</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.621083</td>\n",
       "      <td>3.013673</td>\n",
       "      <td>5.476386</td>\n",
       "      <td>5.48998</td>\n",
       "      <td>10.349507</td>\n",
       "      <td>1.664335</td>\n",
       "      <td>[1.497184, 1.592377, 1.59016, 1.52786, 1.60082...</td>\n",
       "      <td>4.002099</td>\n",
       "      <td>5.483183</td>\n",
       "      <td>7.015772</td>\n",
       "      <td>5000</td>\n",
       "      <td>Passive Replication</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       mean       iqr      cilo     cihi     whishi    whislo  \\\n",
       "0  5.749423  2.992702  5.626825  5.64040  10.372989  1.743512   \n",
       "1  5.621083  3.013673  5.476386  5.48998  10.349507  1.664335   \n",
       "\n",
       "                                              fliers        q1       med  \\\n",
       "0  [1.697545, 1.672757, 1.47343, 1.658997, 1.6082...  4.156760  5.633612   \n",
       "1  [1.497184, 1.592377, 1.59016, 1.52786, 1.60082...  4.002099  5.483183   \n",
       "\n",
       "         q3  rate            algorithm  \n",
       "0  7.149461  5000       No Replication  \n",
       "1  7.015772  5000  Passive Replication  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modes = [\"algorithm\", \"rate\"]\n",
    "warmup=30\n",
    "bucket = \"test99-check\"\n",
    "df_stats, df_stats_run = compute_all(bucket, modes, warmup)\n",
    "df_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stats_run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.catplot(data=df_stats_run, x=\"length\", hue=\"idle-marks.interval\", y=\"whishi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stats_run[df_stats_run[\"rate\"]==\"10000\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "odcit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.tight_layout()\n",
    "df_stats[\"label\"] = df_stats[\"idle-marks.interval\"].astype(\"int\")\n",
    "fig,ax = plt.subplots(4,1,sharex=True,figsize=(5,5))\n",
    "i=0\n",
    "plt.xticks(np.arange(0, 500, 50.0))\n",
    "for i,length in tqdm(enumerate(df_stats[\"length\"].unique())):\n",
    "    if length == \"1\":\n",
    "        ax[i].set_ylabel(\"{} operator\".format(length))\n",
    "    else:\n",
    "        ax[i].set_ylabel(\"{} operators\".format(length))\n",
    "    #ax[i].set_xticks(0,400,50)\n",
    "    ax[i].set_xlim([0,500])\n",
    "    stats = df_stats[(df_stats.length == length)]\n",
    "    if len(stats) == 0:\n",
    "        print(\"Lacking {}\".format(length))\n",
    "    #stats.set_index(\"label\")\n",
    "    #print(stats.to_dict(orient='records'))\n",
    "    odict = stats.to_dict(orient='records',into=OrderedDict)\n",
    "    new_order = [\"TimeMerge\", \"Kazoo\",\"No ordering\",\"Passive Replication\", \"No Replication\"]\n",
    "    stats = []\n",
    "    for d in odict:\n",
    "        stats.append(d)\n",
    "    ax[i].bxp(stats, showfliers=False,vert=False)\n",
    "ax[i].set_xlabel(\"Latency (in ms)\")\n",
    "if \"final\" not in bucket:\n",
    "    fig.text(0.5, 0.5, 'Preversion',\n",
    "             fontsize=80, color='gray',\n",
    "             ha='center', va='center', alpha=0.5, rotation=-45)\n",
    "plt.savefig('results/test.pdf',dpi=300, bbox_inches = \"tight\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# mb2 generic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modes = [\"map-parallelism\", \"algorithm\"]\n",
    "warmup=30\n",
    "bucket = \"mb2-final\"\n",
    "df_stats, df_stats_run = compute_all(bucket, modes, warmup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.catplot(data=df_stats_run, x=\"map-parallelism\", hue=\"algorithm\", y=\"whishi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stats[\"label\"] = df_stats[\"algorithm\"]\n",
    "fig,ax = plt.subplots(4,1,sharex=True,figsize=(5,5))\n",
    "i=0\n",
    "plt.xticks(np.arange(0, 250, 50.0))\n",
    "for i,length in tqdm(enumerate(df_stats[\"map-parallelism\"].unique())):\n",
    "    if length == \"1\":\n",
    "        ax[i].set_ylabel(\"{} operator\".format(length))\n",
    "    else:\n",
    "        ax[i].set_ylabel(\"{} operators\".format(length))\n",
    "    #ax[i].set_xticks(0,400,50)\n",
    "    ax[i].set_xlim([0,250])\n",
    "    stats = df_stats[(df_stats[\"map-parallelism\"] == length)]\n",
    "    if len(stats) == 0:\n",
    "        print(\"Lacking {}\".format(length))\n",
    "    #stats.set_index(\"label\")\n",
    "    #print(stats.to_dict(orient='records'))\n",
    "    odict = stats.to_dict(orient='records',into=OrderedDict)\n",
    "    new_order = [\"TimeMerge\", \"Kazoo\",\"No ordering\",\"Passive Replication\", \"No Replication\"]\n",
    "    stats = []\n",
    "    for el in new_order:\n",
    "        for d in odict:\n",
    "            if d[\"algorithm\"] == el:\n",
    "                stats.append(d)\n",
    "    ax[i].bxp(stats, showfliers=False,vert=False)\n",
    "ax[i].set_xlabel(\"Latency (in ms)\")\n",
    "if \"final\" not in bucket:\n",
    "    fig.text(0.5, 0.5, 'Preversion',\n",
    "             fontsize=80, color='gray',\n",
    "             ha='center', va='center', alpha=0.5, rotation=-45)\n",
    "plt.savefig('results/MB2.pdf',dpi=300, bbox_inches = \"tight\")      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# mb3 generic (fail1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_time(bucket, modes, warmup):\n",
    "    measurements = [o.object_name for o in minio.list_objects_v2(bucket)]\n",
    "    measurement_by_mode = {}\n",
    "    for measurement in tqdm(measurements, desc=\"Metadata\"):\n",
    "        infos = load_metadata(bucket, measurement, modes)\n",
    "        if json.dumps(infos) not in measurement_by_mode:\n",
    "            measurement_by_mode[json.dumps(infos)] = []\n",
    "        measurement_by_mode[json.dumps(infos)].append(measurement)\n",
    "    df_stats_run = []\n",
    "    df_stats = []\n",
    "    array_infos = []\n",
    "    group=None\n",
    "    for group in tqdm(measurement_by_mode.keys(), desc=\"Data\"):\n",
    "        print(group)\n",
    "        map_group = json.loads(group)\n",
    "        array_df = []\n",
    "        for measurement in tqdm(measurement_by_mode[group]):\n",
    "            print(measurement)\n",
    "\n",
    "            df, infos = load_measurement(bucket, measurement, [\"createdS_int\",\"latencyMs\", \"receivedS_int\"])\n",
    "\n",
    "            array_df.append(df)\n",
    "            array_infos.append(infos)\n",
    "            stats = boxplot_stats(df[df[\"createdS_int\"] > warmup].latencyMs, whis=[1,99])[0]\n",
    "            stats[\"measurement\"] = measurement\n",
    "            for key in map_group:\n",
    "                stats[key] = map_group[key]\n",
    "            if \"algorithm\" in stats:\n",
    "                stats[key] = rename_algorithm(stats[key])\n",
    "            df_stats_run.append(stats)\n",
    "            \n",
    "        df_global = pd.concat(array_df)\n",
    "        stats = boxplot_stats(df_global[df_global[\"createdS_int\"] > warmup].latencyMs, whis=[1,99])[0]\n",
    "        for key in map_group:\n",
    "            stats[key] = map_group[key]\n",
    "        if \"algorithm\" in stats:\n",
    "            stats[key] = rename_algorithm(stats[key])\n",
    "        df_stats.append(stats)\n",
    "        \n",
    "    df_stats=pd.DataFrame(df_stats)    \n",
    "    df_stats_run=pd.DataFrame(df_stats_run)  \n",
    "    return array_df, array_infos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modes = [\"path\"]\n",
    "warmup=0\n",
    "#bucket = \"mb3-test-real\"\n",
    "bucket = \"mb3-final\"\n",
    "#bucket = \"mb4b-simple1\"\n",
    "array_df, array_infos = compute_time(bucket, modes, warmup)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array_infos[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "float(array_infos[0][\"planned_kill_date\"]) - float(array_infos[0][\"start_time\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_metric(path, start_time):\n",
    "    df = pd.read_csv(minio.get_object(bucket, path))\n",
    "    df[\"startS\"] = df[\"timestamp\"] - float(start_time)\n",
    "    \n",
    "    return df[df[\"timestamp\"] >= float(start_time)]\n",
    "\n",
    "\n",
    "#import seaborn as sns\n",
    "#sns.relplot(data=all_df, x=\"createdMs\", y=\"latency\",  kind=\"line\", hue=\"path\")\n",
    "length_array = len(array_df)\n",
    "if length_array == 1:\n",
    "    length_array = 2\n",
    "fig,ax = plt.subplots(4, length_array, sharex=True, sharey=\"row\", figsize=(8,8))\n",
    "for i, df in tqdm(enumerate(array_df)):\n",
    "    line=0\n",
    "    colname = rename_algorithm(array_infos[i][\"algorithm\"])\n",
    "    if colname == \"Kazoo\":\n",
    "        colname = \"{} ({})\".format(colname, array_infos[i][\"kazoo_target\"])\n",
    "    ax[line,i].set_title(colname)\n",
    "    #ax[line,i].set_yscale('log')\n",
    "    df.groupby(\"createdS_int\")[\"latencyMs\"].quantile(0.99).reset_index().plot.scatter(x=\"createdS_int\", y=\"latencyMs\", s=1, ax=ax[line, i])\n",
    "    ax[line,i].axvline(array_infos[i][\"killS_other\"], color=\"r\", lw=1)\n",
    "    ax[line,i].set_ylabel(\"Latency\")\n",
    "    ax[line,i].set_ylim([0,1200])\n",
    "    ax[line,i].set_xlim([0,300])\n",
    "\n",
    "    line=1    \n",
    "    df = load_metric(array_infos[i][\"path\"] + \"cpu-seconds-irate.csv\", array_infos[i][\"start_time_initial\"])\n",
    "    df.groupby(\"startS\").sum().reset_index().plot.scatter(x=\"startS\", y=\"value\", s=line,ax=ax[line, i])\n",
    "    ax[line,i].axvline(array_infos[i][\"killS_other\"], color=\"r\", lw=1)\n",
    "    ax[line,i].set_ylabel(\"CPU\")\n",
    "\n",
    "    \n",
    "    #line=2\n",
    "    #df = load_metric(array_infos[i][\"path\"] + \"heap-used.csv\", array_infos[i][\"start_time_initial\"])\n",
    "    #df.groupby(\"startS\").sum().reset_index().plot.scatter(x=\"startS\", y=\"value\", s=1,ax=ax[line, i])\n",
    "    #ax[line,i].axvline(array_infos[i][\"killS\"], color=\"r\")\n",
    "    ##ax[line,i].axvline(array_infos[i][\"killS_other\"], color=\"g\")\n",
    "    #ax[line,i].set_ylabel(\"Memory\")\n",
    "\n",
    "    line=2\n",
    "    df = load_metric(array_infos[i][\"path\"] + \"records-out-per-second.csv\", array_infos[i][\"start_time_initial\"])\n",
    "    df[df[\"task_name\"] == \"map_at_level_0\"].groupby(\"startS\").sum().reset_index().plot.scatter(x=\"startS\", y=\"value\", s=1,ax=ax[line, i])\n",
    "    ax[line,i].axvline(array_infos[i][\"killS_other\"], color=\"r\", lw=1)\n",
    "    #ax[line,i].axvline(array_infos[i][\"killS_other\"], color=\"g\")\n",
    "    ax[line,i].set_ylabel(\"Records out/s (first task)\")\n",
    "\n",
    "    #line=4\n",
    "    #df = load_metric(array_infos[i][\"path\"] + \"records-in-per-second.csv\", array_infos[i][\"start_time_initial\"])\n",
    "    #df[df[\"task_name\"] == \"Sink:_Unnamed\"].groupby(\"startS\").sum().reset_index().plot.scatter(x=\"startS\", y=\"value\", s=1,ax=ax[line, i])\n",
    "    #ax[line,i].axvline(array_infos[i][\"killS\"], color=\"r\")\n",
    "    #ax[line,i].axvline(array_infos[i][\"killS_other\"], color=\"g\")    \n",
    "    #ax[line,i].set_ylabel(\"Records in (sink)\")\n",
    "\n",
    "    line=3\n",
    "    df = load_metric(array_infos[i][\"path\"] + \"num-bytes-out-per-second.csv\", array_infos[i][\"start_time_initial\"])\n",
    "    df.groupby(\"startS\").sum().reset_index().plot.scatter(x=\"startS\", y=\"value\", s=1,ax=ax[line, i])\n",
    "    ax[line,i].axvline(array_infos[i][\"killS_other\"], color=\"r\", lw=1)\n",
    "    \n",
    "    ax[line,i].set_ylabel(\"Bytes in/second (total)\")\n",
    "    ax[line,i].set_xlabel(\"Emission second\")\n",
    "\n",
    "    #df = load_metric(array_infos[i][\"path\"] + \"num-bytes-in-per-second.csv\", array_infos[i][\"start_time_initial\"])\n",
    "    #df.groupby(\"startS\").sum().reset_index().plot.scatter(x=\"startS\", y=\"value\", s=1,ax=ax[6, i])\n",
    "    #ax[6,i].axvline(array_infos[i][\"killS\"], color=\"r\")\n",
    "    #ax[6,i].axvline(array_infos[i][\"killS_other\"], color=\"g\")\n",
    "    #ax[6,i].set_ylabel(\"Bytes out/second (total)\")\n",
    "    #array_df[i].groupby(\"createdS_int\").count().reset_index().plot.scatter(x=\"createdS_int\", y=\"latencyMs\", s=3, ax=ax[7,i])\n",
    "    #array_df[i].groupby(\"receivedS_int\").count().reset_index().plot.scatter(x=\"receivedS_int\", y=\"latencyMs\", s=3, c=\"r\", ax=ax[7,i])   \n",
    "    #ax[7,i].set_ylabel(\"Tasks per second (global)\")\n",
    "if \"final\" not in bucket:\n",
    "    fig.text(0.5, 0.5, 'Preversion',\n",
    "             fontsize=80, color='gray',\n",
    "             ha='center', va='center', alpha=0.5, rotation=-45)\n",
    "plt.savefig('results/MB3.pdf', bbox_inches = \"tight\")       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# generic mb4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modes = [\"idle-marks\", \"algorithm\", \"rate\"]\n",
    "warmup=30\n",
    "bucket = \"mb4-final\"\n",
    "df_stats, df_stats_run = compute_all(bucket, modes, warmup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(algorithm, idlemarks):\n",
    "    string = \"\"\n",
    "    print(algorithm)\n",
    "    if algorithm == \"TimeMerge\":\n",
    "        print(idlemarks)\n",
    "        if idlemarks == \"true\":\n",
    "           string = \" idle-marks\"\n",
    "        else:\n",
    "           string = \" no idle-marks\"\n",
    "    return string\n",
    "\n",
    "df_stats[\"label\"] = df_stats[\"algorithm\"] + df_stats.apply(lambda x:f(x[\"algorithm\"], x[\"idle-marks\"]), axis=1)\n",
    "rates = df_stats[\"rate\"].unique()\n",
    "fig,ax = plt.subplots(len(rates),1,sharex=True,figsize=(5,5))\n",
    "\n",
    "for i,r in enumerate(rates):\n",
    "    \n",
    "    \n",
    "    plt.xticks(np.arange(0, 1500, 100.0))\n",
    "    #ax.set_title(\"Idle marks: {}\".format(length))\n",
    "    #ax[i].set_xlim([0,1000])\n",
    "    ax[i].set_ylabel(\"Rate: {}\".format(r))\n",
    "    odict = df_stats[df_stats[\"rate\"] == r].to_dict(orient='records',into=OrderedDict)\n",
    "    new_order = [\"TimeMerge\", \"Kazoo\",\"No ordering\",\"Passive Replication\", \"No Replication\"]\n",
    "    stats = []\n",
    "    for el in new_order:\n",
    "        for d in odict:\n",
    "            if d[\"algorithm\"] == el:\n",
    "                stats.append(d)\n",
    "    ax[i].bxp(stats, showfliers=False,vert=False)\n",
    "ax[i].set_xlabel(\"Latency (in ms)\")\n",
    "if \"final\" not in bucket:\n",
    "    fig.text(0.5, 0.5, 'Preversion',\n",
    "             fontsize=80, color='gray',\n",
    "             ha='center', va='center', alpha=0.5, rotation=-45)\n",
    "plt.savefig('results/MB4.pdf', bbox_inches = \"tight\")       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# generic mb4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#idle-marks.interval\n",
    "#modes = [\"idle-marks\", \"idle-marks.interval\"]\n",
    "modes = [\"idle-marks\", \"idle-marks.interval\",\"algorithm\"]\n",
    "warmup=30\n",
    "bucket = \"mb4b-simple1\"\n",
    "df_stats, df_stats_run = compute_all(bucket, modes, warmup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stats_run[\"label\"] = df_stats_run[\"idle-marks.interval\"] + \"-\" + df_stats_run[\"algorithm\"]\n",
    "fig,ax = plt.subplots()\n",
    "ax.bxp(df_stats_run.to_dict(orient='records'), showfliers=False,vert=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.catplot(data=df_stats_run, x=\"idle-marks.interval\", y=\"whishi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stats[\"label\"] = df_stats[\"idle-marks\"].apply(lambda x:\"\" if x == \"true\" else \"No idle-marks\")\n",
    "df_stats[\"label\"] = df_stats[\"label\"] + df_stats[\"idle-marks.interval\"].apply(lambda x:\"Interval: {}\".format(x) )\n",
    "df_stats[\"idle-marks.interval\"] = df_stats[\"idle-marks.interval\"].astype(\"int32\")\n",
    "fig,ax = plt.subplots(1,1,sharex=True,figsize=(8,8))\n",
    "i=0\n",
    "plt.xticks(np.arange(0, 1000, 50.0))\n",
    "#ax.set_title(\"Idle marks: {}\".format(length))\n",
    "#ax.set_xlim([0,250])\n",
    "odict = df_stats.sort_values(by='idle-marks.interval', ascending=False).to_dict(orient='records',into=OrderedDict)\n",
    "#new_order = [\"TimeMerge\", \"Kazoo\",\"No ordering\",\"Passive Replication\", \"No Replication\"]\n",
    "#stats = []\n",
    "#for el in new_order:\n",
    "#    for d in odict:\n",
    "#        if d[\"algorithm\"] == el:\n",
    "#            stats.append(d)\n",
    "ax.bxp(odict, showfliers=False,vert=False)\n",
    "ax.set_xlabel(\"Latency (in ms)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# generic mb4''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_time_group(bucket, modes, warmup):\n",
    "    measurements = [o.object_name for o in minio.list_objects_v2(bucket)]\n",
    "    measurement_by_mode = {}\n",
    "    for measurement in tqdm(measurements, desc=\"Metadata\"):\n",
    "        infos = load_metadata(bucket, measurement, modes)\n",
    "        if json.dumps(infos) not in measurement_by_mode:\n",
    "            measurement_by_mode[json.dumps(infos)] = []\n",
    "        measurement_by_mode[json.dumps(infos)].append(measurement)\n",
    "    array_mean = []\n",
    "    array_std = []\n",
    "    \n",
    "    array_count_received = []\n",
    "    array_count_created = []\n",
    "    array_infos = []\n",
    "    group=None\n",
    "    for group in tqdm(measurement_by_mode.keys(), desc=\"Data\"):\n",
    "        map_group = json.loads(group)\n",
    "        array_df = []\n",
    "        array_df_count = []\n",
    "        for measurement in tqdm(measurement_by_mode[group]):\n",
    "\n",
    "            df, infos = load_measurement(bucket, measurement, [\"createdS_int\",\"latencyMs\", \"receivedS_int\"])\n",
    "            df_count = df.groupby(\"createdS_int\").count().reset_index()\n",
    "            array_df.append(df)\n",
    "            array_infos.append(infos)\n",
    "            array_df_count.append(df_count)\n",
    "            \n",
    "        df_global = pd.concat(array_df)\n",
    "        df_global_count = pd.concat(array_df_count)\n",
    "        df_mean = df_global.groupby(\"createdS_int\").quantile(0.99).reset_index()\n",
    "        df_std = df_global.groupby(\"createdS_int\").std().reset_index()\n",
    "        df_count_created = df_global_count.groupby(\"createdS_int\").mean().reset_index() \n",
    "        df_count_received = df_global_count.groupby(\"receivedS_int\").mean().reset_index()\n",
    "        for key in map_group:\n",
    "            df_mean[key] = infos[key]\n",
    "            df_std[key] = infos[key]\n",
    "            df_count_created[key] = infos[key]\n",
    "            df_count_received[key] = infos[key]\n",
    "        \n",
    "        array_mean.append(df_mean)\n",
    "        array_std.append(df_std)\n",
    "        array_count_created.append(df_count_created)\n",
    "        array_count_received.append(df_count_received)\n",
    "\n",
    "    return array_mean, array_std, array_count_created, array_count_received, array_infos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modes = [\"idle-marks.interval\", \"rate2\"]\n",
    "warmup=0\n",
    "bucket = \"mb4c-final\"\n",
    "array_mean, array_std, array_count_created, array_count_received, array_infos = compute_time_group(bucket, modes, warmup)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mean = pd.concat(array_mean)\n",
    "df_std = pd.concat(array_std)\n",
    "def get_throughput_array(infos):\n",
    "    i = 0\n",
    "    x = 0\n",
    "    y = int(infos[\"rate\"])\n",
    "    dots_x = [x]\n",
    "    dots_y = [y]\n",
    "    for xx, value in enumerate(infos[\"pattern1\"].split(\",\")):\n",
    "        dots_x.append(int(value.split(\":\")[0]))\n",
    "        dots_y.append(y)\n",
    "        x = int(value.split(\":\")[0])\n",
    "        y = int(value.split(\":\")[1])\n",
    "        dots_x.append(x)\n",
    "        dots_y.append(y)\n",
    "    dots_x.append(300)\n",
    "    dots_y.append(y)\n",
    "    return dots_x, dots_y\n",
    "    \n",
    "df_mean = pd.concat(array_mean)\n",
    "df_count = pd.concat(array_count_created)\n",
    "rates = df_mean[\"rate2\"].unique()\n",
    "intervals = df_mean[\"idle-marks.interval\"].unique()\n",
    "fig,ax = plt.subplots(len(intervals), len(rates), sharex=True, sharey=\"row\", figsize=(8,8))\n",
    "\n",
    "ar_ax2 = []\n",
    "for i,interval in enumerate(intervals):\n",
    "    for j,r in enumerate(rates):\n",
    "        #df_mean[(df_mean[\"rate2\"] == r) & (df_mean[\"idle-marks.interval\"] == interval)].plot.scatter(x=\"createdS_int\", y=\"latencyMs\", s=3, ax=ax[i])\n",
    "        df_mean[(df_mean[\"rate2\"] == r) & (df_mean[\"idle-marks.interval\"] == interval)].plot.scatter(x=\"createdS_int\", y=\"latencyMs\", s=3, ax=ax[i])\n",
    "        ax2 = ax[i].twinx()\n",
    "        ax[i].set_ylim([0,1250])\n",
    "        ar_ax2.append(ax2)\n",
    "        #df_count[(df_count[\"rate2\"] == r) & (df_count[\"idle-marks.interval\"] == interval)].plot.scatter(x=\"createdS_int\", y=\"latencyMs\", s=3, ax=ax2, c=\"r\")\n",
    "        ax[i].set_ylabel(\"\")\n",
    "        ax2.set_ylabel(\"\")    \n",
    "        ax2.set_ylim([0,1050])\n",
    "        #[0,30,30,60,60,90,90,120,120,150,150,180,180,210,210,240,240,300],[500,500,500,500,250,250,125,125,0,0,125,125,250,250,500,500,1000,1000]\n",
    "        dots_x, dots_y = get_throughput_array(array_infos[i])\n",
    "        ax2.plot(dots_x, dots_y, c=\"g\", linestyle=\"--\")\n",
    "        #if i == 0:\n",
    "        #    ax[0].set_title(\"Basic rate: {}\".format(r))\n",
    "            \n",
    "    if i == 2:\n",
    "        ax[i].set_ylabel(\"Idle marks interval\\n{} ms\".format(interval))\n",
    "        ax2.set_ylabel(\"Throughput variation\")\n",
    "    else:\n",
    "        ax[i].set_ylabel(\"{} ms\".format(interval))\n",
    "plt.setp(ax[-1], xlabel='Event creation second')\n",
    "if \"final\" not in bucket:\n",
    "    fig.text(0.5, 0.5, 'Preversion',\n",
    "             fontsize=80, color='gray',\n",
    "             ha='center', va='center', alpha=0.5, rotation=-45)\n",
    "plt.savefig('results/MB4c.pdf', bbox_inches = \"tight\")     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mean.groupby(\"rate2\").count().reset_index().groupby(\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
