apiVersion: v1
kind: ConfigMap
metadata:
  name: flink-config
  labels:
    app: flink
data:
  flink-conf.yaml: |+
    jobmanager.rpc.address: flink-jobmanager
    taskmanager.numberOfTaskSlots: 1
    blob.server.port: 6124
    jobmanager.rpc.port: 6123
    taskmanager.rpc.port: 6122
    jobmanager.heap.size: 1024m
    taskmanager.memory.process.size: 1024m
    metrics.reporters: prom
    metrics.reporter.prom.class: org.apache.flink.metrics.prometheus.PrometheusReporter
    env.java.opts: -agentlib:jdwp=transport=dt_socket,server=y,suspend=n,address=5005

  log4j.properties: |+
    log4j.rootLogger=INFO, file
    log4j.logger.akka=INFO
    log4j.logger.org.apache.kafka=INFO
    log4j.logger.org.apache.hadoop=INFO
    log4j.logger.org.apache.zookeeper=INFO
    log4j.appender.file=org.apache.log4j.FileAppender
    log4j.appender.file.file=${log.file}
    log4j.appender.file.layout=org.apache.log4j.PatternLayout
    log4j.appender.file.layout.ConversionPattern=%d{yyyy-MM-dd HH:mm:ss,SSS} %-5p %-60c %x - %m%n
    log4j.logger.org.apache.flink.shaded.akka.org.jboss.netty.channel.DefaultChannelPipeline=ERROR, file
    #log4j.logger.org.apache.flink.streaming.runtime.io.OrderingService=DEBUG
    #log4j.logger.org.apache.flink.streaming.runtime.io.replication.KafkaReplication=TRACE
    #log4j.logger.org.apache.flink.streaming.runtime.io.replication.KafkaOrderBroadcaster=TRACE
    #log4j.logger.org.apache.flink.streaming.runtime.io.replication.LiveRobinAlgorithm=TRACE
    #log4j.logger.org.apache.flink.streaming.api.operators.AbstractStreamOperator=TRACE
  core-site.xml: |+
    <?xml version="1.0" encoding="UTF-8"?>
    <?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
    <configuration>
      <property>
        <name>hadoop.security.authentication</name>
        <value>simple</value>
      </property>
      <property>
        <name>hadoop.security.authorization</name>
        <value>false</value>
      </property>
    </configuration>
  
  hdfs-site.xml: |+
    <?xml version="1.0" encoding="UTF-8"?>
    <?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
    <configuration>
      <property>
            <name>dfs.client.block.write.replace-datanode-on-failure.enable</name>
            <value>false</value>
        </property>
      <property>
            <name>dfs.client.block.write.replace-datanode-on-failure.policy</name>
            <value>NEVER</value>
        </property>
    </configuration>